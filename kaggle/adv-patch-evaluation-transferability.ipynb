{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12055613,"sourceType":"datasetVersion","datasetId":7587385},{"sourceId":12065974,"sourceType":"datasetVersion","datasetId":7594620},{"sourceId":12103799,"sourceType":"datasetVersion","datasetId":7619993},{"sourceId":12115928,"sourceType":"datasetVersion","datasetId":7628551},{"sourceId":12115954,"sourceType":"datasetVersion","datasetId":7628572},{"sourceId":12191159,"sourceType":"datasetVersion","datasetId":7678902},{"sourceId":12228573,"sourceType":"datasetVersion","datasetId":7704641},{"sourceId":246463271,"sourceType":"kernelVersion"},{"sourceId":256318189,"sourceType":"kernelVersion"},{"sourceId":90869,"sourceType":"modelInstanceVersion","modelInstanceId":76178,"modelId":100857},{"sourceId":426091,"sourceType":"modelInstanceVersion","modelInstanceId":347342,"modelId":368598},{"sourceId":436552,"sourceType":"modelInstanceVersion","modelInstanceId":356065,"modelId":377364},{"sourceId":436561,"sourceType":"modelInstanceVersion","modelInstanceId":356073,"modelId":377372},{"sourceId":452455,"sourceType":"modelInstanceVersion","modelInstanceId":367024,"modelId":387928},{"sourceId":522308,"sourceType":"modelInstanceVersion","modelInstanceId":410613,"modelId":428459},{"sourceId":522363,"sourceType":"modelInstanceVersion","modelInstanceId":410663,"modelId":428511},{"sourceId":524155,"sourceType":"modelInstanceVersion","modelInstanceId":411587,"modelId":429404},{"sourceId":524190,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":411601,"modelId":429418}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!rm -rf /kaggle/working/adversarial-patch-transferability\n!rm -rf /kaggle/working/adversarial-patch-transferability/sam2\n!rm -rf /kaggle/working/adversarial-patch-transferability/sam2/sam2\n!rm -rf /kaggle/working/sam2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T08:05:32.603278Z","iopub.execute_input":"2025-08-29T08:05:32.603502Z","iopub.status.idle":"2025-08-29T08:05:33.061272Z","shell.execute_reply.started":"2025-08-29T08:05:32.603487Z","shell.execute_reply":"2025-08-29T08:05:33.059890Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!git clone -b noiseinit https://github.com/AGAMPANDEYY/adversarial-patch-transferability.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T08:05:33.987977Z","iopub.execute_input":"2025-08-29T08:05:33.988241Z","iopub.status.idle":"2025-08-29T08:05:35.034032Z","shell.execute_reply.started":"2025-08-29T08:05:33.988218Z","shell.execute_reply":"2025-08-29T08:05:35.033275Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'adversarial-patch-transferability'...\nremote: Enumerating objects: 589, done.\u001b[K\nremote: Counting objects: 100% (104/104), done.\u001b[K\nremote: Compressing objects: 100% (55/55), done.\u001b[K\nremote: Total 589 (delta 72), reused 49 (delta 49), pack-reused 485 (from 2)\u001b[K\nReceiving objects: 100% (589/589), 7.32 MiB | 24.99 MiB/s, done.\nResolving deltas: 100% (313/313), done.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%cd ..\n!ls\n%cd sam2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd adversarial-patch-transferability\n!git fetch origin noiseinit\n!git pull origin noiseinit","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T05:26:42.116874Z","iopub.execute_input":"2025-08-29T05:26:42.117486Z","iopub.status.idle":"2025-08-29T05:26:43.001238Z","shell.execute_reply.started":"2025-08-29T05:26:42.117462Z","shell.execute_reply":"2025-08-29T05:26:43.000346Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/adversarial-patch-transferability\nremote: Enumerating objects: 9, done.\u001b[K\nremote: Counting objects: 100% (9/9), done.\u001b[K\nremote: Compressing objects: 100% (5/5), done.\u001b[K\nremote: Total 5 (delta 3), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\nUnpacking objects: 100% (5/5), 1.41 KiB | 1.41 MiB/s, done.\nFrom https://github.com/AGAMPANDEYY/adversarial-patch-transferability\n * branch            noiseinit  -> FETCH_HEAD\n   53185a2..390030c  noiseinit  -> origin/noiseinit\nFrom https://github.com/AGAMPANDEYY/adversarial-patch-transferability\n * branch            noiseinit  -> FETCH_HEAD\nUpdating 53185a2..390030c\nFast-forward\n pretrained_models/SegFormer/model.py | 33 \u001b[32m++++++++++++++++++++\u001b[m\u001b[31m-------------\u001b[m\n 1 file changed, 20 insertions(+), 13 deletions(-)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!git clone https://github.com/facebookresearch/sam2.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T08:05:37.791138Z","iopub.execute_input":"2025-08-29T08:05:37.791700Z","iopub.status.idle":"2025-08-29T08:05:42.539905Z","shell.execute_reply.started":"2025-08-29T08:05:37.791665Z","shell.execute_reply":"2025-08-29T08:05:42.539112Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'sam2'...\nremote: Enumerating objects: 1070, done.\u001b[K\nremote: Total 1070 (delta 0), reused 0 (delta 0), pack-reused 1070 (from 1)\u001b[K\nReceiving objects: 100% (1070/1070), 128.11 MiB | 40.09 MiB/s, done.\nResolving deltas: 100% (380/380), done.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%cd sam2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T08:05:42.541519Z","iopub.execute_input":"2025-08-29T08:05:42.541847Z","iopub.status.idle":"2025-08-29T08:05:42.548416Z","shell.execute_reply.started":"2025-08-29T08:05:42.541821Z","shell.execute_reply":"2025-08-29T08:05:42.547628Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/sam2\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install --q -e .","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T08:05:42.549363Z","iopub.execute_input":"2025-08-29T08:05:42.549703Z","iopub.status.idle":"2025-08-29T08:11:00.654071Z","shell.execute_reply.started":"2025-08-29T08:05:42.549678Z","shell.execute_reply":"2025-08-29T08:11:00.653070Z"}},"outputs":[{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building editable for SAM-2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!mkdir -p checkpoints/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T08:11:00.656111Z","iopub.execute_input":"2025-08-29T08:11:00.656349Z","iopub.status.idle":"2025-08-29T08:11:00.777476Z","shell.execute_reply.started":"2025-08-29T08:11:00.656325Z","shell.execute_reply":"2025-08-29T08:11:00.776546Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T08:11:00.778363Z","iopub.execute_input":"2025-08-29T08:11:00.778577Z","iopub.status.idle":"2025-08-29T08:11:00.900424Z","shell.execute_reply.started":"2025-08-29T08:11:00.778552Z","shell.execute_reply":"2025-08-29T08:11:00.899444Z"}},"outputs":[{"name":"stdout","text":"assets\t\t    docker-compose.yaml  pyproject.toml    setup.py\nbackend.Dockerfile  INSTALL.md\t\t README.md\t   tools\ncheckpoints\t    LICENSE\t\t RELEASE_NOTES.md  training\nCODE_OF_CONDUCT.md  LICENSE_cctorch\t sam2\nCONTRIBUTING.md     MANIFEST.in\t\t SAM_2.egg-info\ndemo\t\t    notebooks\t\t sav_dataset\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!curl -L https://dl.fbaipublicfiles.com/sam2/checkpoints/sam2_hiera_tiny.pt \\\n      -o checkpoints/sam2_hiera_tiny.pt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T08:11:00.901630Z","iopub.execute_input":"2025-08-29T08:11:00.901896Z","iopub.status.idle":"2025-08-29T08:11:01.306074Z","shell.execute_reply.started":"2025-08-29T08:11:00.901868Z","shell.execute_reply":"2025-08-29T08:11:01.305089Z"}},"outputs":[{"name":"stdout","text":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   111    0   111    0     0    437      0 --:--:-- --:--:-- --:--:--   438\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!pwd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T08:11:01.307358Z","iopub.execute_input":"2025-08-29T08:11:01.307656Z","iopub.status.idle":"2025-08-29T08:11:01.424394Z","shell.execute_reply.started":"2025-08-29T08:11:01.307627Z","shell.execute_reply":"2025-08-29T08:11:01.423740Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/sam2\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!ls checkpoints","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T08:11:01.425226Z","iopub.execute_input":"2025-08-29T08:11:01.425415Z","iopub.status.idle":"2025-08-29T08:11:01.543984Z","shell.execute_reply.started":"2025-08-29T08:11:01.425395Z","shell.execute_reply":"2025-08-29T08:11:01.543127Z"}},"outputs":[{"name":"stdout","text":"download_ckpts.sh  sam2_hiera_tiny.pt\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Testing on PIDNet-S, PIDNet-M, BiSeNetv2, ICNet","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport sys\nimport gc\nsys.path.append('/kaggle/working/adversarial-patch-transferability')\nfrom utils.utils import setup_logging, get_config_from_yaml, process_config, print_config\nfrom dataset.cityscapes import Cityscapes\nfrom metrics.performance import SegmentationMetric\nfrom utils.helper import val_plot\nfrom patch.create import Patch\n#from metrics.performance import SegmentationMetric\n\n\nfrom pretrained_models.ICNet.icnet import ICNet\nfrom pretrained_models.BisNetV2.model import BiSeNetV2\nfrom pretrained_models.PIDNet.model import PIDNet, get_pred_model\nfrom sam2.sam2_image_predictor import SAM2ImagePredictor\nfrom sam2.build_sam import build_sam2\n\nimport pickle\nfrom copy import deepcopy\n# from trainer.trainer import Trainer\n# import torch\nfrom tqdm import tqdm\nconfig = get_config_from_yaml('/kaggle/working/adversarial-patch-transferability/configs/config.yaml')\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T08:11:01.545113Z","iopub.execute_input":"2025-08-29T08:11:01.545335Z","iopub.status.idle":"2025-08-29T08:11:10.277805Z","shell.execute_reply.started":"2025-08-29T08:11:01.545311Z","shell.execute_reply":"2025-08-29T08:11:10.276965Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import pickle\npidnet_s_p = pickle.load(open( \"/kaggle/input/patch-2/pidnet_s (2).p\", \"rb\" ))[0]\n\npidnet_s_p = pickle.load(open( \"/kaggle/input/noiseinit_kldivergence/pytorch/default/1/pidnet_s.p\", \"rb\" ))[0]\npidnet_s_p = pickle.load(open( \"/kaggle/input/ensemble_transeg/pytorch/default/1/pidnet_s (2).p\", \"rb\" ))[0]\npatches = {\n    'pidnet_s':pidnet_s_p\n}\n\nfor patch in patches:\n  patches[patch] = patches[patch].to(device)\n  print(patch)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T08:11:10.279654Z","iopub.execute_input":"2025-08-29T08:11:10.279971Z","iopub.status.idle":"2025-08-29T08:11:10.582756Z","shell.execute_reply.started":"2025-08-29T08:11:10.279952Z","shell.execute_reply":"2025-08-29T08:11:10.581469Z"}},"outputs":[{"name":"stdout","text":"pidnet_s\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Pytorch optimised patch","metadata":{}},{"cell_type":"code","source":"import torch\n\n# Load the .pt file\npidnet_s_p = torch.load(\"/kaggle/input/patch_greedy/pytorch/default/1/optimized_patch.pt\")\n\n# Put patch into a dictionary (if you want to keep the same structure)\npatches = {\n    'pidnet_s': pidnet_s_p\n}\n\n# Move to device\nfor patch in patches:\n    patches[patch] = patches[patch].to(device)\n    print(patch)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cityscape_val = Cityscapes(\n          root = config.dataset.root,\n          list_path = config.dataset.val,\n          num_classes = config.dataset.num_classes,\n          multi_scale = False,\n          flip = False,\n          ignore_label = config.train.ignore_label,\n          base_size = config.train.base_size,\n          crop_size = (config.train.height,config.train.width),\n        )\n\nval_dataloader = torch.utils.data.DataLoader(dataset=cityscape_val,\n                                            batch_size=1,\n                                            shuffle=True,\n                                            num_workers=config.train.num_workers,\n                                            pin_memory=config.train.pin_memory,\n                                            drop_last=config.train.drop_last)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T08:11:10.583713Z","iopub.execute_input":"2025-08-29T08:11:10.584033Z","iopub.status.idle":"2025-08-29T08:11:10.617139Z","shell.execute_reply.started":"2025-08-29T08:11:10.584004Z","shell.execute_reply":"2025-08-29T08:11:10.616593Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"## PIDNet-s\nmodel = torch.load('/kaggle/input/pidnet_s_cityscapes_test/pytorch/default/1/PIDNet_S_Cityscapes_test.pt',map_location=device)\npidnet_s = get_pred_model(name = 'pidnet_s', num_classes = 19).to(device)\nif 'state_dict' in model:\n    model = model['state_dict']\nmodel_dict = pidnet_s.state_dict()\nmodel = {k[6:]: v for k, v in model.items() # k[6:] to start after model. in key names\n                    if k[6:] in model_dict.keys()}\n\npidnet_s.load_state_dict(model)\npidnet_s.eval()\nprint('PIDNet-s Model loaded')\n\n## PIDNet-m\nmodel = torch.load('/kaggle/input/pidnet_m_cityscapes_test/pytorch/default/1/PIDNet_M_Cityscapes_test.pt',map_location=device)\npidnet_m = get_pred_model(name = 'pidnet_m', num_classes = 19).to(device)\nif 'state_dict' in model:\n    model = model['state_dict']\nmodel_dict = pidnet_m.state_dict()\nmodel = {k[6:]: v for k, v in model.items() # k[6:] to start after model. in key names\n                    if k[6:] in model_dict.keys()}\n\npidnet_m.load_state_dict(model)\npidnet_m.eval()\nprint('PIDNet-m Model loaded')\n\n## PIDNet-l\ntry:\n    model = torch.load('/kaggle/input/pidnet-l-weights/PIDNet_L_Cityscapes_test.pt',\n                       map_location=device)\n    pidnet_l = get_pred_model(name='pidnet_l', num_classes=19).to(device)\n    if 'state_dict' in model:\n        model = model['state_dict']\n    model_dict = pidnet_l.state_dict()\n    model = {k[6:]: v for k, v in model.items() if k[6:] in model_dict.keys()}\n    pidnet_l.load_state_dict(model)\n    pidnet_l.eval()\n    print('PIDNet-L loaded')\nexcept Exception as e:\n    pidnet_l = None\n    print(f'WARNING: PIDNet-L not loaded: {e}')\n\n# ---- metrics for new models ----\nmetric_pidnet_l   = SegmentationMetric(config)\n\n# ## ICNet\n# model = torch.load('/content/drive/MyDrive/Colab Notebooks/1_Papers/3_Attack_generation/pretrained_models/ICNet/Copy of resnet50_2024-12-22 08:52:50 EST-0500_176_0.661.pth.tar',map_location=device)\n# icnet = ICNet(nclass = 19).to(device)\n# icnet.load_state_dict(model['model_state_dict'])\n# icnet.eval()\n# print('ICNet loaded')\n\n## BiseNetV2\nmodel = torch.load('/kaggle/input/bisenetv2_cityscapes_test/pytorch/default/1/model_final_v2_city.pth',map_location=device)\nbisenetv2 = BiSeNetV2(19,aux_mode = 'eval').to(device)\nbisenetv2.load_state_dict(model, strict=False)\nbisenetv2.eval()\nprint('BisNetV2 loaded')\n\nimport torch\nfrom omegaconf import OmegaConf\nfrom hydra.utils import instantiate\nfrom sam2.sam2_image_predictor import SAM2ImagePredictor\n\ndevice  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ncfg      = OmegaConf.load(\"/kaggle/input/segment-anything-2/pytorch/sam2-hiera-tiny/1/sam2_hiera_t.yaml\")\nckpt     = torch.load(\"/kaggle/input/segment-anything-2/pytorch/sam2-hiera-tiny/1/sam2_hiera_tiny.pt\", map_location=device)\n\n# 1) instantiate the bare model\nsam2_model = instantiate(cfg.model).to(device)\n\n# 2) load its weights\n#   checkpoint dict usually has a \"model\" key\nsam2_model.load_state_dict(ckpt.get(\"model\", ckpt), strict=False)\n\n# 3) wrap in predictor\npredictor = SAM2ImagePredictor(sam2_model)\nprint(\"SAM2 tiny loaded via manual state_dict!\")\n\n## segformer from huggingface\nimport os, io, contextlib\nfrom transformers.utils import logging as hf_logging\nfrom transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n\n# (A) kill progress bars & tokenizers chatter\nos.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# (B) silence HF logging completely\nhf_logging.set_verbosity_error()           # or set_verbosity(hf_logging.CRITICAL)\nhf_logging.disable_default_handler()       # prevent adding console handlers\nhf_logging.enable_propagation()            # keep logs from re-adding handlers\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# (C) suppress any stdout/stderr printed during load (belt & suspenders)\nwith contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):\n    processor = SegformerImageProcessor.from_pretrained(\n        \"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\"\n    )\n    segformer = SegformerForSemanticSegmentation.from_pretrained(\n        \"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\"\n    )\n\nsegformer.to(device).eval()\nprint(\"segformer loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T08:19:40.547756Z","iopub.execute_input":"2025-08-29T08:19:40.548499Z","iopub.status.idle":"2025-08-29T08:19:44.016480Z","shell.execute_reply.started":"2025-08-29T08:19:40.548470Z","shell.execute_reply":"2025-08-29T08:19:44.015803Z"}},"outputs":[{"name":"stdout","text":"PIDNet-s Model loaded\nPIDNet-m Model loaded\nPIDNet-L loaded\nBisNetV2 loaded\nSAM2 tiny loaded via manual state_dict!\nsegformer loaded\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import numpy as np\nimport torch\n\nIMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1)\nIMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1)\n\n@torch.no_grad()\ndef hf_segformer_forward(segformer_model, hf_processor, x_bchw, already_normalized=False):\n    \"\"\"\n    x_bchw: torch.FloatTensor [B,3,H,W]\n    Returns: logits tensor [B, 19, H/4, W/4] for Cityscapes model.\n    \"\"\"\n    x = x_bchw.detach().to(segformer_model.device)\n\n    # If caller gave ImageNet-normalized tensors, revert to 0..1 so the HF processor can normalize once.\n    if already_normalized:\n        x = (x * IMAGENET_STD.to(x.device)) + IMAGENET_MEAN.to(x.device)\n        x = x.clamp(0, 1)\n\n    # Convert to uint8 images for processor\n    imgs = (x.clamp(0,1) * 255.0).round().to(torch.uint8)          # [B,3,H,W]\n    imgs = imgs.permute(0,2,3,1).cpu().numpy().astype(np.uint8)     # [B,H,W,3]\n\n    inputs = hf_processor(images=list(imgs), return_tensors='pt', do_resize=False)\n    pixel_values = inputs['pixel_values'].to(segformer_model.device)\n    out = segformer_model(pixel_values=pixel_values)  # SemanticSegmenterOutput\n    return out.logits  # tensor [B, num_labels, H/4, W/4]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T08:25:52.399791Z","iopub.execute_input":"2025-08-29T08:25:52.400145Z","iopub.status.idle":"2025-08-29T08:25:52.408039Z","shell.execute_reply.started":"2025-08-29T08:25:52.400117Z","shell.execute_reply":"2025-08-29T08:25:52.407424Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"### With adversarial patch","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom copy import deepcopy\nfrom tqdm import tqdm\nimport gc\nfrom tabulate import tabulate\n\nmean_standard = np.array([0.485, 0.456, 0.406], dtype=np.float32)\nstd_standard = np.array([0.229, 0.224, 0.225], dtype=np.float32)\nx = (2048 - 200) // 2\ny = (1024 - 200) // 2\nx_end = x + 200\ny_end = y + 200\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Initialize metrics for all models\nmetric_pidnet_s = SegmentationMetric(config)\nmetric_pidnet_m = SegmentationMetric(config)\nmetric_pidnet_l = SegmentationMetric(config)\nmetric_bisenetv2 = SegmentationMetric(config)\nmetric_sam2 = SegmentationMetric(config)\nmetric_segformer = SegmentationMetric(config)\n\n# Dictionary to store mIoU results\ndata = {}\n\n# Iterate over patches\nfor pat in ['pidnet_s']:  # Modify as needed to include other patches\n    patch = patches[pat]\n    \n    # Reset all metrics\n    metric_pidnet_s.reset()\n    metric_pidnet_m.reset()\n    metric_pidnet_l.reset()\n    metric_bisenetv2.reset()\n    metric_sam2.reset()\n    metric_segformer.reset()\n    \n    print(f'Computing for patch: {pat}')\n    \n    for iter, batches in tqdm(enumerate(val_dataloader)):\n        image_standard, label, _, _, _ = batches\n        label_patched = deepcopy(label)\n\n        # Apply adversarial patch\n        image_standard[:, :, y:y_end, x:x_end] = patch\n        label_patched[:, y:y_end, x:x_end] = config.train.ignore_label\n        \n        # Move to device\n        image_standard = image_standard.to(device)\n        label_patched = label_patched.to(device)\n\n        # PIDNet-S\n        outputs = pidnet_s(image_standard)\n        output = F.interpolate(\n            outputs[config.test.output_index_pidnet], \n            size=label.shape[-2:],\n            mode='bilinear', \n            align_corners=True\n        )\n        metric_pidnet_s.update(output, label_patched)\n\n        # PIDNet-M\n        outputs = pidnet_m(image_standard)\n        output = F.interpolate(\n            outputs[config.test.output_index_pidnet], \n            size=label.shape[-2:],\n            mode='bilinear', \n            align_corners=True\n        )\n        metric_pidnet_m.update(output, label_patched)\n\n        # PIDNet-L\n        outputs = pidnet_l(image_standard)\n        output = F.interpolate(\n            outputs[config.test.output_index_pidnet], \n            size=label.shape[-2:],\n            mode='bilinear', \n            align_corners=True\n        )\n        metric_pidnet_l.update(output, label_patched)\n\n        # BiseNetV2\n        outputs = bisenetv2(image_standard)\n        output = outputs[config.test.output_index_bisenet]\n        metric_bisenetv2.update(output, label_patched)\n\n        # SAM2\n        img_np = image_standard.squeeze(0).permute(1, 2, 0).cpu().numpy()\n        img_np = (img_np * std_standard + mean_standard) * 255.0\n        img_np = img_np.clip(0, 255).astype(np.uint8)\n        \n        predictor.set_image(img_np)\n        H, W, _ = img_np.shape\n        pts = np.array([[W // 2, H // 2]])\n        labels = np.array([1])\n        masks, _, _ = predictor.predict(\n            point_coords=pts, \n            point_labels=labels, \n            multimask_output=False\n        )\n        pred_mask = torch.from_numpy(masks[0]).unsqueeze(0).unsqueeze(0).to(device).float()\n        metric_sam2.update(pred_mask, label_patched)\n\n        # SegFormer\n        with torch.no_grad():\n            logits = hf_segformer_forward(\n                segformer_model = segformer,\n                hf_processor    = processor,\n                x_bchw          = image_standard,\n                already_normalized = True   # set False if your images are raw 0..1\n            )\n            # Upsample to GT size and update your metric\n            output = F.interpolate(\n                logits, size=label.shape[-2:], mode='bilinear', align_corners=True\n            )\n            metric_segformer.update(output, label_patched)\n\n        # Clean up\n        del outputs, output, image_standard, label, batches\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    # Store mIoU results\n    data[pat] = {\n        'PIDNet-S': metric_pidnet_s.get()[1],\n        'PIDNet-M': metric_pidnet_m.get()[1],\n        'PIDNet-L': metric_pidnet_l.get()[1],\n        'BiseNetV2': metric_bisenetv2.get()[1],\n        'SAM2': metric_sam2.get()[1],\n        'SegFormer': metric_segformer.get()[1]\n    }\n\n# Create and print formatted table\nheaders = ['Patch', 'PIDNet-S', 'PIDNet-M', 'PIDNet-L', 'BiseNetV2', 'SAM2', 'SegFormer']\ntable_data = []\nfor pat, metrics in data.items():\n    row = [pat] + [f\"{metrics[model]:.4f}\" for model in ['PIDNet-S', 'PIDNet-M', 'PIDNet-L', 'BiseNetV2', 'SAM2', 'SegFormer']]\n    table_data.append(row)\n\ntable = tabulate(table_data, headers=headers, tablefmt='pipe', floatfmt='.4f')\nprint(\"\\nModel mIoU Results:\")\nprint(table)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T08:26:16.833792Z","iopub.execute_input":"2025-08-29T08:26:16.834305Z","iopub.status.idle":"2025-08-29T08:50:24.739861Z","shell.execute_reply.started":"2025-08-29T08:26:16.834280Z","shell.execute_reply":"2025-08-29T08:50:24.739018Z"}},"outputs":[{"name":"stdout","text":"Computing for patch: pidnet_s\n","output_type":"stream"},{"name":"stderr","text":"500it [24:07,  2.90s/it]","output_type":"stream"},{"name":"stdout","text":"\nModel mIoU Results:\n| Patch    |   PIDNet-S |   PIDNet-M |   PIDNet-L |   BiseNetV2 |   SAM2 |   SegFormer |\n|:---------|-----------:|-----------:|-----------:|------------:|-------:|------------:|\n| pidnet_s |     0.8170 |     0.8537 |     0.8931 |      0.6830 | 0.0196 |      0.8082 |\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"from tqdm import tqdm\nfrom copy import deepcopy\nmean_standard = np.array([0.485, 0.456, 0.406], dtype=np.float32)\nstd_standard = np.array([0.229, 0.224, 0.225], dtype=np.float32)\nx = (2048 - 200) // 2  # 924\ny = (1024 - 200) // 2  # 412\nx_end = x + 200\ny_end = y + 200\n\n# Initialize metric\nmetric_segformer = SegmentationMetric(config)  # Assuming config is defined\n\n# Load SegFormer\nsegformer_b0 = load_segformer_local(\n    variant_or_name=\"segformer_b0\",\n    ckpt_path=ckpt,  # Replace with your checkpoint path\n    device=device,\n    num_classes=config.dataset.num_classes,  # Likely 19 for Cityscapes\n    decoder_channels=256,\n    strict=True,\n    verbose=True\n)\nsegformer_b0.eval()\n\n# Evaluation loop\ndata = {}\nfor pat in ['pidnet_s']:  # Single patch for testing\n    patch = patches[pat]  # Assuming patches is a dict with patch tensors\n    # Normalize patch if in [0, 255]\n    if patch.max() > 1.0:\n        patch = (patch / 255.0 - mean_standard) / std_standard\n    metric_segformer.reset()\n    print(f'Computing for: {pat}')\n    for iter, batches in tqdm(enumerate(val_dataloader)):\n        image_standard, label, _, _, _ = batches\n        label_patched = deepcopy(label)\n\n        # Add adversarial patch\n        image_standard[:, :, y:y_end, x:x_end] = patch\n        label_patched[:, y:y_end, x:x_end] = config.train.ignore_label\n\n        # Move to device\n        image_standard = image_standard.to(device)\n        label_patched = label_patched.to(device)\n\n        # SegFormer inference\n        with torch.no_grad():\n            outputs = segformer_logits(segformer_b0, image_standard)  # [B, 19, H/4, W/4]\n            output = F.interpolate(\n                outputs, size=label.shape[-2:], mode='bilinear', align_corners=True\n            )\n            metric_segformer.update(output, label_patched)\n\n        del outputs, output, image_standard, label, batches\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    data[pat] = metric_segformer.get()[1]  # Get mIoU\n    print(f'Results [SegFormer]: {data[pat]}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T08:26:01.941847Z","iopub.execute_input":"2025-08-29T08:26:01.942646Z","iopub.status.idle":"2025-08-29T08:26:02.058024Z","shell.execute_reply.started":"2025-08-29T08:26:01.942608Z","shell.execute_reply":"2025-08-29T08:26:02.056929Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_check_seekable\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    850\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'seek'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3383076599.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Load SegFormer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m segformer_b0 = load_segformer_local(\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mvariant_or_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"segformer_b0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Replace with your checkpoint path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/adversarial-patch-transferability/pretrained_models/SegFormer/model.py\u001b[0m in \u001b[0;36mload_segformer_local\u001b[0;34m(variant_or_name, ckpt_path, device, num_classes, decoder_channels, strict, verbose)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    754\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_open_buffer_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_open_buffer_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected 'r' or 'w' in mode but got {mode}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, buffer)\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m         \u001b[0m_check_seekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_check_seekable\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m         \u001b[0mraise_err_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"seek\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mraise_err_msg\u001b[0;34m(patterns, e)\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0;34m+\u001b[0m \u001b[0;34m\" try to load from it instead.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m                 )\n\u001b[0;32m--> 847\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead."],"ename":"AttributeError","evalue":"'dict' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.","output_type":"error"}],"execution_count":25},{"cell_type":"markdown","source":"## Random patch for sssegformer ","metadata":{}},{"cell_type":"code","source":"mean_standard = np.array([0.485, 0.456, 0.406], dtype=np.float32)\nstd_standard = np.array([0.229, 0.224, 0.225], dtype=np.float32)\nx = (2048 - 200) // 2  # 924\ny = (1024 - 200) // 2  # 412\nx_end = x + 200\ny_end = y + 200\n\n# Initialize metric\nmetric_segformer = SegmentationMetric(config)  # Assuming config is defined\n\n# Load SegFormer\nsegformer_b0 = load_segformer_local(\n    variant_or_name=\"segformer_b0\",\n    ckpt_path=ckpt,  # Replace with your checkpoint path\n    device=device,\n    num_classes=config.dataset.num_classes,  # Likely 19 for Cityscapes\n    decoder_channels=256,\n    strict=True,\n    verbose=True\n)\nsegformer_b0.eval()\n\n# Evaluation loop with random patch\nmetric_segformer.reset()\nprint('Computing for random patch')\nfor iter, batches in tqdm(enumerate(val_dataloader)):\n    image_standard, label, _, _, _ = batches\n    label_patched = deepcopy(label)\n\n    # Generate random patch and normalize\n    patch = torch.rand(3, 200, 200)  # Values in [0, 1]\n    patch = (patch - mean_standard.reshape(3, 1, 1)) / std_standard.reshape(3, 1, 1) # Normalize to match image_standard\n    image_standard[:, :, y:y_end, x:x_end] = patch\n    label_patched[:, y:y_end, x:x_end] = config.train.ignore_label\n\n    # Move to device\n    image_standard = image_standard.to(device)\n    label_patched = label_patched.to(device)\n\n    # SegFormer inference\n    with torch.no_grad():\n        outputs = segformer_logits(segformer_b0, image_standard)  # [B, 19, H/4, W/4]\n        output = F.interpolate(\n            outputs, size=label.shape[-2:], mode='bilinear', align_corners=True\n        )\n        metric_segformer.update(output, label_patched)\n\n    del outputs, output, image_standard, label, batches\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# Print result\nprint(f'SegFormer mIoU: {metric_segformer.get()[1]}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T05:41:08.213314Z","iopub.execute_input":"2025-08-29T05:41:08.214094Z","iopub.status.idle":"2025-08-29T05:45:06.557979Z","shell.execute_reply.started":"2025-08-29T05:41:08.214070Z","shell.execute_reply":"2025-08-29T05:45:06.557100Z"}},"outputs":[{"name":"stdout","text":"[SegFormer] matched 191/191 keys | missing 0 | unexpected 0\nComputing for random patch\n","output_type":"stream"},{"name":"stderr","text":"500it [03:58,  2.10it/s]","output_type":"stream"},{"name":"stdout","text":"SegFormer mIoU: 0.016241049394011497\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"### With random patch","metadata":{}},{"cell_type":"code","source":"mean_standard = np.array([0.485, 0.456, 0.406],dtype = np.float32)\nstd_standard = np.array([0.229, 0.224, 0.225],dtype = np.float32)\nx = (2048 - 200)//2\ny = (1024 - 200)//2\nx_end = x + 200\ny_end = y + 200\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmetric_pidnet_m = SegmentationMetric(config)\nmetric_pidnet_s = SegmentationMetric(config)\n# metric_icnet = SegmentationMetric(config)\nmetric_bisenetv2 = SegmentationMetric(config)\nmetric_sam2 = SegmentationMetric(config)\n\nmetric_pidnet_m.reset()\nmetric_pidnet_s.reset()\n# metric_icnet.reset()\nmetric_bisenetv2.reset()\nmetric_sam2.reset()\n\nfor iter,batches in tqdm(enumerate(val_dataloader)):\n  image_standard,label,_,_,_ = batches\n  label_patched = deepcopy(label)\n\n  ## random patch\n  patch = torch.rand(3,200,200)\n  image_standard[:,:, y:y_end, x:x_end] = patch\n  label_patched[:, y:y_end, x:x_end] = config.train.ignore_label\n  ## On device\n  image_standard = image_standard.to(device)\n  label_patched = label_patched.to(device)\n\n  ##PIDNet-m\n  outputs = pidnet_m(image_standard)\n  size = label.shape\n  output = F.interpolate(\n                outputs[config.test.output_index_pidnet], size[-2:],\n                mode='bilinear', align_corners=True\n                        )\n  metric_pidnet_m.update(output, label_patched)\n\n  ## PIDNet-s\n  outputs = pidnet_s(image_standard)\n  size = label.shape\n  output = F.interpolate(\n                outputs[config.test.output_index_pidnet], size[-2:],\n                mode='bilinear', align_corners=True\n                        )\n  metric_pidnet_s.update(output, label_patched)\n\n  # ##ICNet\n  # outputs = icnet(image_standard)\n  # output = outputs[config.test.output_index_icnet]\n  # metric_icnet.update(output,label_patched)\n\n  ##BiseNetV2\n  outputs = bisenetv2(image_standard)\n  output = outputs[config.test.output_index_bisenet]\n  metric_bisenetv2.update(output,label_patched)\n\n    # run SAM 2 prediction\n  predictor.set_image(img_np)\n    # use a center-point prompt\n  H, W, _ = img_np.shape\n  pts = np.array([[W//2, H//2]])\n  labels = np.array([1])\n  masks, _, _ = predictor.predict(point_coords=pts, point_labels=labels, multimask_output=False)\n    \n    # convert to torch tensor and update metric\n  pred_mask = torch.from_numpy(masks[0])       # H×W\n  pred_mask = torch.from_numpy(masks[0]).unsqueeze(0).unsqueeze(0).to(device).float()\n  metric_sam2.update(pred_mask, label_patched)\n\n  del outputs,output,image_standard,label,batches\n  gc.collect()\n  torch.cuda.empty_cache()\n\nprint(f'PIDNet-M MIOU: {metric_pidnet_m.get()[1]}')\nprint(f'PIDNet-S MIOU: {metric_pidnet_s.get()[1]}')\nprint(f'BisNetV2 MIOU: {metric_bisenetv2.get()[1]}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f'SAM2 MIOU: {metric_sam2.get()[1]}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Without Patch","metadata":{}},{"cell_type":"code","source":"mean_standard = np.array([0.485, 0.456, 0.406], dtype=np.float32)\nstd_standard  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\ndevice        = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Reset metrics\nmetric_pidnet_m   = SegmentationMetric(config)\nmetric_pidnet_s   = SegmentationMetric(config)\nmetric_bisenetv2  = SegmentationMetric(config)\nmetric_sam2       = SegmentationMetric(config)\n\n# Single key for loop consistency (you can also just drop the outer loop entirely)\nfor pat in ['no_patch']:\n    metric_pidnet_m.reset()\n    metric_pidnet_s.reset()\n    metric_bisenetv2.reset()\n    metric_sam2.reset()\n    print(f'Computing on unmodified images:')\n\n    for batches in tqdm(val_dataloader, total=len(val_dataloader)):\n        image, label, *_ = batches\n\n        # send to device\n        image = image.to(device)\n        label = label.to(device)\n\n        # --- PIDNet-m ---\n        out_m = pidnet_m(image)\n        out_m = F.interpolate(\n            out_m[config.test.output_index_pidnet],\n            size=label.shape[-2:], mode='bilinear', align_corners=True\n        )\n        metric_pidnet_m.update(out_m, label)\n\n        # --- PIDNet-s ---\n        out_s = pidnet_s(image)\n        out_s = F.interpolate(\n            out_s[config.test.output_index_pidnet],\n            size=label.shape[-2:], mode='bilinear', align_corners=True\n        )\n        metric_pidnet_s.update(out_s, label)\n\n        # --- BiseNetV2 ---\n        out_b = bisenetv2(image)[config.test.output_index_bisenet]\n        metric_bisenetv2.update(out_b, label)\n\n        # --- SAM-2 evaluation on the full image ---\n        # convert to H×W×3 uint8 for SAM\n        img_np = image.squeeze(0).permute(1,2,0).cpu().numpy()\n        img_np = (img_np * std_standard + mean_standard) * 255.0\n        img_np = img_np.clip(0,255).astype(np.uint8)\n\n        predictor.set_image(img_np)\n        H, W, _ = img_np.shape\n        pts      = np.array([[W//2, H//2]])\n        lbls     = np.array([1])\n        masks, _, _ = predictor.predict(\n            point_coords=pts,\n            point_labels=lbls,\n            multimask_output=False\n        )\n        # [1,1,H,W]\n        pred_mask = (\n            torch.from_numpy(masks[0])\n                 .unsqueeze(0).unsqueeze(0)\n                 .to(device).float()\n        )\n        metric_sam2.update(pred_mask, label)\n\n        # cleanup\n        del out_m, out_s, out_b, image, label, masks, img_np\n        torch.cuda.empty_cache()\n\n    # report\n    data = [\n        metric_pidnet_m.get()[1],\n        metric_pidnet_s.get()[1],\n        metric_bisenetv2.get()[1],\n        metric_sam2.get()[1]\n    ]\n    print(f'Results [PIDNet-M, PIDNet-S, BiseNetV2, SAM-2]: {data}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## New","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/SargamG/adversarial-patch-transferability","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cp /kaggle/input/pidnet-s-weights/PIDNet_S_Cityscapes_test.pt /kaggle/working/adversarial-patch-transferability/pretrained_models/PIDNet\n!cp /kaggle/input/pidnet-m-weights/PIDNet_M_Cityscapes_test.pt /kaggle/working/adversarial-patch-transferability/pretrained_models/PIDNet\n!cp /kaggle/input/bisenetv2-weights/bisenetv2.pth /kaggle/working/adversarial-patch-transferability/pretrained_models/BisNetV2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working/adversarial-patch-transferability","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pretrained_models.ICNet.icnet import ICNet\n#config = get_config_from_yaml('/kaggle/working/adversarial-patch-transferability/configs/config.yaml')\nicnet = ICNet(nclass = 19).to(device)\nfor name, _ in icnet.named_modules():\n    print(name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for name, _ in bisenetv2.named_modules():\n    print(name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for name, _ in pidnet_m.named_modules():\n    print(name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for name, _ in pidnet_s.named_modules():\n    print(name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cityscape_val = Cityscapes(\n          root = config.dataset.root,\n          list_path = config.dataset.val,\n          num_classes = config.dataset.num_classes,\n          multi_scale = False,\n          flip = False,\n          ignore_label = config.train.ignore_label,\n          base_size = config.train.base_size,\n          crop_size = (config.train.height,config.train.width),\n        )\n\nval_dataloader = torch.utils.data.DataLoader(dataset=cityscape_val,\n                                            batch_size=1,\n                                            shuffle=True,\n                                            num_workers=config.train.num_workers,\n                                            pin_memory=config.train.pin_memory,\n                                            drop_last=config.train.drop_last)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cityscape_train = Cityscapes(\n          root = config.dataset.root,\n          list_path = config.dataset.train,\n          num_classes = config.dataset.num_classes,\n          multi_scale = config.train.multi_scale,\n          flip = config.train.flip,\n          ignore_label = config.train.ignore_label,\n          base_size = config.train.base_size,\n          crop_size = (config.train.height,config.train.width),\n          scale_factor = config.train.scale_factor\n        )\ntrain_dataloader = torch.utils.data.DataLoader(dataset=cityscape_train,\n                                              batch_size=1,\n                                              shuffle=True,\n                                              num_workers=config.train.num_workers,\n                                              pin_memory=config.train.pin_memory,\n                                              drop_last=config.train.drop_last)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def hook(module, input, output):\n    global feature_maps\n    feature_maps = output\n    feature_maps.retain_grad()\nlayer_name = 'pretrained.layer2.3.relu'\nfor name, module in icnet.named_modules():\n    if name == layer_name:\n        module.register_forward_hook(hook)\n        break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for iter,batches in tqdm(enumerate(train_dataloader)):\n    image_standard,label,_,_,_, idx = batches\n\n    ## On device\n    image_standard = image_standard.to(device)\n    label = label.to(device)\n\n    ##PIDNet-m\n    outputs = icnet(image_standard)\n    break\nfeature_maps.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport sys\nsys.path.append('/kaggle/working/adversarial-patch-transferability')\nfrom utils.utils import setup_logging, get_config_from_yaml, process_config, print_config\nfrom trainer.trainer import PatchTrainer\nimport torch\nimport pickle","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = 'pidnet_s'\n## getting the config file\nconfig = get_config_from_yaml('configs/config.yaml',model = model)\n\n## processing config to initialize directories for logs etc\nmain_logger = process_config(config)\n\n## training\ntrain_obj = PatchTrainer(config,main_logger,model)\ntrain_obj.register_forward_hook()\nH = train_obj.get_agg_gradient()\nprint(H)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"path='/kaggle/working/logs/H.pt'\ntorch.save(H,path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport gzip\npath = '/kaggle/working/logs/H_tensor.pt.gz'\nwith gzip.open(path, 'wb') as f:\n    torch.save(H, f)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import yaml\n\nyaml_path = \"/kaggle/working/adversarial-patch-transferability/configs/config.yaml\"\n\n# Read\nwith open(yaml_path, 'r') as file:\n    conf = yaml.safe_load(file)\n\nprint(conf)  # View content\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"conf[\"train\"][\"batch_size\"]=1\nwith open(yaml_path, 'w') as file:\n    yaml.safe_dump(conf, file)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = 'pidnet_s'\n## getting the config file\nconfig = get_config_from_yaml('configs/config.yaml',model = model)\n\n## processing config to initialize directories for logs etc\nmain_logger = process_config(config)\n\n## training\ntrain_obj = PatchTrainer(config,main_logger,model)\ntrain_obj.register_forward_hook()\nH","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save = train_obj.train(H)\npickle.dump( save, open(config.experiment.log_patch_address+config.model.name+\".p\", \"wb\" ) )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pickle.dump( save, open(config.experiment.log_patch_address+config.model.name+\"_bbfa\"+\".p\", \"wb\" ) )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport sys\nimport gc\nsys.path.append('/kaggle/working/adversarial-patch-transferability')\nfrom utils.utils import setup_logging, get_config_from_yaml, process_config, print_config\nfrom dataset.cityscapes import Cityscapes\nfrom metrics.performance import SegmentationMetric\nfrom utils.helper import val_plot\nfrom patch.create import Patch\n#from metrics.performance import SegmentationMetric\n\n\nfrom pretrained_models.ICNet.icnet import ICNet\nfrom pretrained_models.BisNetV2.model import BiSeNetV2\nfrom pretrained_models.PIDNet.model import PIDNet, get_pred_model\nimport pickle\nfrom copy import deepcopy\n# from trainer.trainer import Trainer\n# import torch\nfrom tqdm import tqdm\nconfig = get_config_from_yaml('/kaggle/working/adversarial-patch-transferability/configs/config.yaml')\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pidnet_s_p = pickle.load(open( \"/kaggle/working/adversarial-patch-transferability/Experiments/pidnet_s_bbfa.p\", \"rb\" ))[0]\npatches = {\n    'pidnet_s':pidnet_s_p\n}\n\nfor patch in patches:\n  patches[patch] = patches[patch].to(device)\n  print(patch)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cityscape_val = Cityscapes(\n          root = config.dataset.root,\n          list_path = config.dataset.val,\n          num_classes = config.dataset.num_classes,\n          multi_scale = False,\n          flip = False,\n          ignore_label = config.train.ignore_label,\n          base_size = config.train.base_size,\n          crop_size = (config.train.height,config.train.width),\n        )\n\nval_dataloader = torch.utils.data.DataLoader(dataset=cityscape_val,\n                                            batch_size=1,\n                                            shuffle=True,\n                                            num_workers=config.train.num_workers,\n                                            pin_memory=config.train.pin_memory,\n                                            drop_last=config.train.drop_last)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## PIDNet-s\nmodel = torch.load('/kaggle/working/adversarial-patch-transferability/pretrained_models/PIDNet/PIDNet_S_Cityscapes_test.pt',map_location=device)\npidnet_s = get_pred_model(name = 'pidnet_s', num_classes = 19).to(device)\nif 'state_dict' in model:\n    model = model['state_dict']\nmodel_dict = pidnet_s.state_dict()\nmodel = {k[6:]: v for k, v in model.items() # k[6:] to start after model. in key names\n                    if k[6:] in model_dict.keys()}\n\npidnet_s.load_state_dict(model)\npidnet_s.eval()\nprint('PIDNet Model loaded')\n\n## PIDNet-m\nmodel = torch.load('/kaggle/working/adversarial-patch-transferability/pretrained_models/PIDNet/PIDNet_M_Cityscapes_test.pt',map_location=device)\npidnet_m = get_pred_model(name = 'pidnet_m', num_classes = 19).to(device)\nif 'state_dict' in model:\n    model = model['state_dict']\nmodel_dict = pidnet_m.state_dict()\nmodel = {k[6:]: v for k, v in model.items() # k[6:] to start after model. in key names\n                    if k[6:] in model_dict.keys()}\n\npidnet_m.load_state_dict(model)\npidnet_m.eval()\nprint('PIDNet Model loaded')\n\n# ## ICNet\n# model = torch.load('/content/drive/MyDrive/Colab Notebooks/1_Papers/3_Attack_generation/pretrained_models/ICNet/Copy of resnet50_2024-12-22 08:52:50 EST-0500_176_0.661.pth.tar',map_location=device)\n# icnet = ICNet(nclass = 19).to(device)\n# icnet.load_state_dict(model['model_state_dict'])\n# icnet.eval()\n# print('ICNet loaded')\n\n## BiseNetV2\nmodel = torch.load('/kaggle/working/adversarial-patch-transferability/pretrained_models/BisNetV2/bisenetv2.pth',map_location=device)\nbisenetv2 = BiSeNetV2(19,aux_mode = 'eval').to(device)\nbisenetv2.load_state_dict(model, strict=False)\nbisenetv2.eval()\nprint('BisNetV2 loaded')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mean_standard = np.array([0.485, 0.456, 0.406],dtype = np.float32)\nstd_standard = np.array([0.229, 0.224, 0.225],dtype = np.float32)\nx = (2048 - 200)//2\ny = (1024 - 200)//2\nx_end = x + 200\ny_end = y + 200\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmetric_pidnet_m = SegmentationMetric(config)\nmetric_pidnet_s = SegmentationMetric(config)\n# metric_icnet = SegmentationMetric(config)\nmetric_bisenetv2 = SegmentationMetric(config)\n\n\ndata = {}\nfor pat in ['pidnet_s']:\n  patch = patches[pat]\n  metric_pidnet_m.reset()\n  metric_pidnet_s.reset()\n  # metric_icnet.reset()\n  metric_bisenetv2.reset()\n  print(f'Computing for: {pat}')\n  temp = []\n  for iter,batches in tqdm(enumerate(val_dataloader)):\n    image_standard,label,_,_,_,_ = batches\n    label_patched = deepcopy(label)\n\n    ## adding patch\n    image_standard[:,:, y:y_end, x:x_end] = patch\n    label_patched[:, y:y_end, x:x_end] = config.train.ignore_label\n    ## On device\n    image_standard = image_standard.to(device)\n    label_patched = label_patched.to(device)\n\n    ##PIDNet-m\n    outputs = pidnet_m(image_standard)\n    size = label.shape\n    output = F.interpolate(\n                  outputs[config.test.output_index_pidnet], size[-2:],\n                  mode='bilinear', align_corners=True\n                          )\n    metric_pidnet_m.update(output, label_patched)\n\n    ## PIDNet-s\n    outputs = pidnet_s(image_standard)\n    size = label.shape\n    output = F.interpolate(\n                  outputs[config.test.output_index_pidnet], size[-2:],\n                  mode='bilinear', align_corners=True\n                          )\n    metric_pidnet_s.update(output, label_patched)\n\n    # ##ICNet\n    # outputs = icnet(image_standard)\n    # output = outputs[config.test.output_index_icnet]\n    # metric_icnet.update(output,label_patched)\n\n    ##BiseNetV2\n    outputs = bisenetv2(image_standard)\n    output = outputs[config.test.output_index_bisenet]\n    metric_bisenetv2.update(output,label_patched)\n\n    del outputs,output,image_standard,label,batches\n    gc.collect()\n    torch.cuda.empty_cache()\n\n  data[pat] = [\n      metric_pidnet_m.get()[1],\n      metric_pidnet_s.get()[1],\n      # metric_icnet.get()[1],\n      metric_bisenetv2.get()[1],\n  ]\n  print(data[pat])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualising feature maps after trained patch","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport sys\nimport gc\nsys.path.append('/kaggle/working/adversarial-patch-transferability')\nfrom utils.utils import setup_logging, get_config_from_yaml, process_config, print_config\nfrom dataset.cityscapes import Cityscapes\nfrom metrics.performance import SegmentationMetric\nfrom utils.helper import val_plot\nfrom patch.create import Patch\n#from metrics.performance import SegmentationMetric\n\n\nfrom pretrained_models.ICNet.icnet import ICNet\nfrom pretrained_models.BisNetV2.model import BiSeNetV2\nfrom pretrained_models.PIDNet.model import PIDNet, get_pred_model\nimport pickle\nfrom copy import deepcopy\n# from trainer.trainer import Trainer\n# import torch\nfrom tqdm import tqdm\nconfig = get_config_from_yaml('/kaggle/working/adversarial-patch-transferability/configs/config.yaml')\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cityscape_val = Cityscapes(\n          root = config.dataset.root,\n          list_path = config.dataset.val,\n          num_classes = config.dataset.num_classes,\n          multi_scale = False,\n          flip = False,\n          ignore_label = config.train.ignore_label,\n          base_size = config.train.base_size,\n          crop_size = (config.train.height,config.train.width),\n        )\n\nval_dataloader = torch.utils.data.DataLoader(dataset=cityscape_val,\n                                            batch_size=1,\n                                            shuffle=False,\n                                            num_workers=config.train.num_workers,\n                                            pin_memory=config.train.pin_memory,\n                                            drop_last=config.train.drop_last)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## PIDNet-s\nmodel = torch.load('/kaggle/working/adversarial-patch-transferability/pretrained_models/PIDNet/PIDNet_S_Cityscapes_test.pt',map_location=device)\npidnet_s = get_pred_model(name = 'pidnet_s', num_classes = 19).to(device)\nif 'state_dict' in model:\n    model = model['state_dict']\nmodel_dict = pidnet_s.state_dict()\nmodel = {k[6:]: v for k, v in model.items() # k[6:] to start after model. in key names\n                    if k[6:] in model_dict.keys()}\n\npidnet_s.load_state_dict(model)\npidnet_s.eval()\nprint('PIDNet Model loaded')\n\n## PIDNet-m\nmodel = torch.load('/kaggle/working/adversarial-patch-transferability/pretrained_models/PIDNet/PIDNet_M_Cityscapes_test.pt',map_location=device)\npidnet_m = get_pred_model(name = 'pidnet_m', num_classes = 19).to(device)\nif 'state_dict' in model:\n    model = model['state_dict']\nmodel_dict = pidnet_m.state_dict()\nmodel = {k[6:]: v for k, v in model.items() # k[6:] to start after model. in key names\n                    if k[6:] in model_dict.keys()}\n\npidnet_m.load_state_dict(model)\npidnet_m.eval()\nprint('PIDNet Model loaded')\n\n# ## ICNet\n# model = torch.load('/content/drive/MyDrive/Colab Notebooks/1_Papers/3_Attack_generation/pretrained_models/ICNet/Copy of resnet50_2024-12-22 08:52:50 EST-0500_176_0.661.pth.tar',map_location=device)\n# icnet = ICNet(nclass = 19).to(device)\n# icnet.load_state_dict(model['model_state_dict'])\n# icnet.eval()\n# print('ICNet loaded')\n\n## BiseNetV2\nmodel = torch.load('/kaggle/working/adversarial-patch-transferability/pretrained_models/BisNetV2/bisenetv2.pth',map_location=device)\nbisenetv2 = BiSeNetV2(19,aux_mode = 'eval').to(device)\nbisenetv2.load_state_dict(model, strict=False)\nbisenetv2.eval()\nprint('BisNetV2 loaded')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_maps_attack = []\nfeature_maps = []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(feature_maps_attack))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def hook_attack(module, input, output):\n    feature_maps_attack.append(output.detach().cpu()) \n\ndef hook(module, input, output):\n    feature_maps.append(output.detach().cpu())\n    \nlayer_name = 'layer3.2.bn2'\nfor name, module in pidnet_m.named_modules():\n    if name == layer_name:\n        module.register_forward_hook(hook_attack)\n        break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pidnet_s_p = pickle.load(open( \"/kaggle/working/adversarial-patch-transferability/Experiments/pidnet_s_bbfa.p\", \"rb\" ))[0]\npatches = {\n    'pidnet_s':pidnet_s_p\n}\n\nfor patch in patches:\n  patches[patch] = patches[patch].to(device)\n  print(patch)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mean_standard = np.array([0.485, 0.456, 0.406],dtype = np.float32)\nstd_standard = np.array([0.229, 0.224, 0.225],dtype = np.float32)\nx = (2048 - 200)//2\ny = (1024 - 200)//2\nx_end = x + 200\ny_end = y + 200\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nfor pat in ['pidnet_s']:\n  patch = patches[pat]\n  print(f'Computing for: {pat}')\n  for iter,batches in tqdm(enumerate(val_dataloader)):\n    image_standard,label,_,_,_,idx = batches\n    label_patched = deepcopy(label)\n\n    ## adding patch\n    image_standard[:,:, y:y_end, x:x_end] = patch\n    label_patched[:, y:y_end, x:x_end] = config.train.ignore_label\n    ## On device\n    image_standard = image_standard.to(device)\n    label_patched = label_patched.to(device)\n\n    ##PIDNet-m\n    outputs = pidnet_m(image_standard)\n    size = label.shape\n    output = F.interpolate(\n                  outputs[config.test.output_index_pidnet], size[-2:],\n                  mode='bilinear', align_corners=True\n                          )\n    break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mean_standard = np.array([0.485, 0.456, 0.406],dtype = np.float32)\nstd_standard = np.array([0.229, 0.224, 0.225],dtype = np.float32)\nx = (2048 - 200)//2\ny = (1024 - 200)//2\nx_end = x + 200\ny_end = y + 200\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nfor iter,batches in tqdm(enumerate(val_dataloader)):\n  image_standard,label,_,_,_,_ = batches\n  label_patched = deepcopy(label)\n\n  ## random patch\n  patch = torch.rand(3,200,200)\n  image_standard[:,:, y:y_end, x:x_end] = patch\n  label_patched[:, y:y_end, x:x_end] = config.train.ignore_label\n  ## On device\n  image_standard = image_standard.to(device)\n  label_patched = label_patched.to(device)\n\n  ##PIDNet-s\n  outputs = pidnet_s(image_standard)\n  size = label.shape\n  output = F.interpolate(\n                outputs[config.test.output_index_pidnet], size[-2:],\n                mode='bilinear', align_corners=True\n                        )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(feature_maps_attack))\nprint(len(feature_maps))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## for layer 3\nimport matplotlib.pyplot as plt\n\n# Get the first feature map batch\nfm = feature_maps_attack[0][0]  # shape: [C, H, W], get first image in batch\n\n# Show first 8 channels\nplt.figure(figsize=(16, 8))\nfor i in range(8):\n    plt.subplot(2, 4, i+1)\n    plt.imshow(fm[i], cmap='viridis')\n    plt.axis('off')\n    plt.title(f'Channel {i}')\nplt.suptitle(\"Feature Maps from Layer after Adversarial Patch\")\nplt.tight_layout()\nplt.show()\n## On pidnet-s","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## for layer 3\nimport matplotlib.pyplot as plt\n\n# Get the first feature map batch\nfm = feature_maps[0][0]  # shape: [C, H, W], get first image in batch\n\n# Show first 8 channels\nplt.figure(figsize=(16, 8))\nfor i in range(8):\n    plt.subplot(2, 4, i+1)\n    plt.imshow(fm[i], cmap='viridis')\n    plt.axis('off')\n    plt.title(f'Channel {i}')\nplt.suptitle(\"Feature Maps from Layer after Random Patch on PIDNet-S\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## For layer 1\nimport matplotlib.pyplot as plt\n\n# Get the first feature map batch\nfm = feature_maps_attack[0][0]  # shape: [C, H, W], get first image in batch\n\n# Show first 8 channels\nplt.figure(figsize=(16, 8))\nfor i in range(8):\n    plt.subplot(2, 4, i+1)\n    plt.imshow(fm[i], cmap='viridis')\n    plt.axis('off')\n    plt.title(f'Channel {i}')\nplt.suptitle(\"Feature Maps from Layer after Adversarial Patch\")\nplt.tight_layout()\nplt.show()\n## On pidnet-s","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## For layer 1\nimport matplotlib.pyplot as plt\n\n# Get the first feature map batch\nfm = feature_maps[0][0]  # shape: [C, H, W], get first image in batch\n\n# Show first 8 channels\nplt.figure(figsize=(16, 8))\nfor i in range(8):\n    plt.subplot(2, 4, i+1)\n    plt.imshow(fm[i], cmap='viridis')\n    plt.axis('off')\n    plt.title(f'Channel {i}')\nplt.suptitle(\"Feature Maps from Layer after Random Patch on PIDNet-S\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## For layer 1\nimport matplotlib.pyplot as plt\n\n# Get the first feature map batch\nfm = feature_maps_attack[0][0]  # shape: [C, H, W], get first image in batch\n\n# Show first 8 channels\nplt.figure(figsize=(16, 8))\nfor i in range(8):\n    plt.subplot(2, 4, i+1)\n    plt.imshow(fm[i], cmap='viridis')\n    plt.axis('off')\n    plt.title(f'Channel {i}')\nplt.suptitle(\"Feature Maps from Layer after Adversarial Patch\")\nplt.tight_layout()\nplt.show()\n## On pidnet-m","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## For layer 3\nimport matplotlib.pyplot as plt\n\n# Get the first feature map batch\nfm = feature_maps_attack[0][0]  # shape: [C, H, W], get first image in batch\n\n# Show first 8 channels\nplt.figure(figsize=(16, 8))\nfor i in range(8):\n    plt.subplot(2, 4, i+1)\n    plt.imshow(fm[i], cmap='viridis')\n    plt.axis('off')\n    plt.title(f'Channel {i}')\nplt.suptitle(\"Feature Maps from Layer after Adversarial Patch\")\nplt.tight_layout()\nplt.show()\n## On pidnet-m","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_maps = []\n\ndef hook_fn(module, input, output):\n    feature_maps.append(output.detach().cpu())  # Save to CPU to avoid memory issues\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"layer_name = 'layer3.2.bn2'  # replace as needed\n\n# Find and hook the layer\nfor name, module in model.model.named_modules():  # assuming your model is `model.model`\n    if name == layer_name:\n        module.register_forward_hook(hook_fn)\n        break\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save[1].shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# If file is in the current directory\npath='/kaggle/working/adversarial-patch-transferability/trainer/trainer.py'\nwith open(path, 'r') as file:\n    contents = file.read()\n\nprint(contents)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Comparing feature maps","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport sys\nimport gc\nsys.path.append('/kaggle/working/adversarial-patch-transferability')\nfrom utils.utils import setup_logging, get_config_from_yaml, process_config, print_config\nfrom dataset.cityscapes import Cityscapes\nfrom metrics.performance import SegmentationMetric\nfrom utils.helper import val_plot\nfrom patch.create import Patch\n#from metrics.performance import SegmentationMetric\n\n\nfrom pretrained_models.ICNet.icnet import ICNet\nfrom pretrained_models.BisNetV2.model import BiSeNetV2\nfrom pretrained_models.PIDNet.model import PIDNet, get_pred_model\nimport pickle\nfrom copy import deepcopy\n# from trainer.trainer import Trainer\n# import torch\nfrom tqdm import tqdm\nconfig = get_config_from_yaml('/kaggle/working/adversarial-patch-transferability/configs/config.yaml')\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cityscape_val = Cityscapes(\n          root = config.dataset.root,\n          list_path = config.dataset.val,\n          num_classes = config.dataset.num_classes,\n          multi_scale = False,\n          flip = False,\n          ignore_label = config.train.ignore_label,\n          base_size = config.train.base_size,\n          crop_size = (config.train.height,config.train.width),\n        )\n\nval_dataloader = torch.utils.data.DataLoader(dataset=cityscape_val,\n                                            batch_size=1,\n                                            shuffle=False,\n                                            num_workers=config.train.num_workers,\n                                            pin_memory=config.train.pin_memory,\n                                            drop_last=config.train.drop_last)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## PIDNet-s\nmodel = torch.load('/kaggle/working/adversarial-patch-transferability/pretrained_models/PIDNet/PIDNet_S_Cityscapes_test.pt',map_location=device)\npidnet_s = get_pred_model(name = 'pidnet_s', num_classes = 19).to(device)\nif 'state_dict' in model:\n    model = model['state_dict']\nmodel_dict = pidnet_s.state_dict()\nmodel = {k[6:]: v for k, v in model.items() # k[6:] to start after model. in key names\n                    if k[6:] in model_dict.keys()}\n\npidnet_s.load_state_dict(model)\npidnet_s.eval()\nprint('PIDNet Model loaded')\n\n## PIDNet-m\nmodel = torch.load('/kaggle/working/adversarial-patch-transferability/pretrained_models/PIDNet/PIDNet_M_Cityscapes_test.pt',map_location=device)\npidnet_m = get_pred_model(name = 'pidnet_m', num_classes = 19).to(device)\nif 'state_dict' in model:\n    model = model['state_dict']\nmodel_dict = pidnet_m.state_dict()\nmodel = {k[6:]: v for k, v in model.items() # k[6:] to start after model. in key names\n                    if k[6:] in model_dict.keys()}\n\npidnet_m.load_state_dict(model)\npidnet_m.eval()\nprint('PIDNet Model loaded')\n\n# ## ICNet\n# model = torch.load('/content/drive/MyDrive/Colab Notebooks/1_Papers/3_Attack_generation/pretrained_models/ICNet/Copy of resnet50_2024-12-22 08:52:50 EST-0500_176_0.661.pth.tar',map_location=device)\n# icnet = ICNet(nclass = 19).to(device)\n# icnet.load_state_dict(model['model_state_dict'])\n# icnet.eval()\n# print('ICNet loaded')\n\n## BiseNetV2\nmodel = torch.load('/kaggle/working/adversarial-patch-transferability/pretrained_models/BisNetV2/bisenetv2.pth',map_location=device)\nbisenetv2 = BiSeNetV2(19,aux_mode = 'eval').to(device)\nbisenetv2.load_state_dict(model, strict=False)\nbisenetv2.eval()\nprint('BisNetV2 loaded')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pidnet_s_p = pickle.load(open( \"/kaggle/working/adversarial-patch-transferability/Experiments/pidnet_s_bbfa.p\", \"rb\" ))[0]\npatches = {\n    'pidnet_s':pidnet_s_p\n}\n\nfor patch in patches:\n  patches[patch] = patches[patch].to(device)\n  print(patch)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_maps_pidnet_s =torch.randn(64,32,64)\nfeature_maps_pidnet_m = torch.randn(128,32,64)\nfeature_maps_bisenetv2 = torch.randn(128,32,64)\n\ndef hook_pidnet_s(module, input, output):\n    feature_maps_pidnet_s = output\n\ndef hook_pidnet_m(module, input, output):\n    feature_maps_pidnet_m = output\n\ndef hook_bisenetv2(module, input, output):\n    feature_maps_bisenetv2 = output\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target_layer1 = 'compression4.1'  # for PIDNet-S, e.g.\ntarget_layer2 = 'compression4.1'  # for BiSeNet\ntarget_layer3 = 'segment.S5_5.conv_last.relu'  # for ResNet-like\n\nfor name, module in pidnet_s.named_modules():\n    if name == target_layer1:\n        module.register_forward_hook(hook_pidnet_s)\n\nfor name, module in pidnet_m.named_modules():\n    if name == target_layer2:\n        module.register_forward_hook(hook_pidnet_m)\n\nfor name, module in bisenetv2.named_modules():\n    if name == target_layer3:\n        module.register_forward_hook(hook_bisenetv2)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mean_standard = np.array([0.485, 0.456, 0.406],dtype = np.float32)\nstd_standard = np.array([0.229, 0.224, 0.225],dtype = np.float32)\nx = (2048 - 200)//2\ny = (1024 - 200)//2\nx_end = x + 200\ny_end = y + 200\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndata = {}\ntemp = []\nfor iter,batches in tqdm(enumerate(val_dataloader)):\n    image_standard,label,_,_,_,_ = batches\n    \n    ## On device\n    image_standard = image_standard.to(device)\n    label = label.to(device)\n    \n    ##PIDNet-m\n    outputs = pidnet_m(image_standard)\n    size = label.shape\n    output = F.interpolate(\n                  outputs[config.test.output_index_pidnet], size[-2:],\n                  mode='bilinear', align_corners=True\n                          )\n    \n    ## PIDNet-s\n    outputs = pidnet_s(image_standard)\n    size = label.shape\n    output = F.interpolate(\n                  outputs[config.test.output_index_pidnet], size[-2:],\n                  mode='bilinear', align_corners=True\n                          )\n    \n    # ##ICNet\n    # outputs = icnet(image_standard)\n    # output = outputs[config.test.output_index_icnet]\n    # metric_icnet.update(output,label_patched)\n    \n    ##BiseNetV2\n    outputs = bisenetv2(image_standard)\n    output = outputs[config.test.output_index_bisenet]\n    \n    del outputs,output,image_standard,label,batches\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(feature_maps_pidnet_s.shape)\nprint(feature_maps_pidnet_m.shape)\nprint(feature_maps_bisenetv2.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F\n\n# Resize function (to common shape: C=64, H=64, W=64)\ndef resize_feature_map(fmap, target_channels=64, target_size=(64, 128)):\n    fmap = fmap.unsqueeze(0)  # (1, C, H, W)\n    \n    # First: spatial resize\n    fmap = F.interpolate(fmap, size=target_size, mode='bilinear', align_corners=False)\n\n    # Then: channel resize using 1x1 conv if needed\n    if fmap.shape[1] != target_channels:\n        conv = torch.nn.Conv2d(fmap.shape[1], target_channels, kernel_size=1)\n        fmap = conv(fmap)\n    \n    return fmap  # (1,C, H, W)\n\n# Resize all to (64, 64, 64)\npidnet_s_resized = resize_feature_map(feature_maps_pidnet_s, 128, (32,64))\npidnet_m_resized = resize_feature_map(feature_maps_pidnet_m, 128, (32,64))\nbisenetv2_resized = resize_feature_map(feature_maps_bisenetv2, 128, (32,64))\nprint(pidnet_s_resized.shape)\n# Flatten to vectors\nfm1 = pidnet_s_resized.flatten()\nfm2 = pidnet_m_resized.flatten()\nfm3 = bisenetv2_resized.flatten()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fm1.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute cosine similarity between corresponding channels\ndef mean_cosine_similarity(fmA, fmB):\n    sim = F.cosine_similarity(fmA, fmB, dim=0)  # channel-wise\n    return sim.mean().item()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"PIDNet-S vs PIDNet-M:\", mean_cosine_similarity(fm1, fm2))\nprint(\"PIDNet-S vs BiSeNetv2:\", mean_cosine_similarity(fm1, fm3))\nprint(\"PIDNet-M vs BiSeNetv2:\", mean_cosine_similarity(fm2, fm3))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Comparing feature maps using random and adversarial patch on PIDNet-S","metadata":{}},{"cell_type":"code","source":"feature_maps_pidnet_s = []\nfeature_maps_pidnet_s_attack = []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def hook_pidnet_s(module, input, output):\n    feature_maps_pidnet_s.append(output.detach().cpu())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target_layer1 = 'layer3.2.bn2'  # for PIDNet-S\n\nfor name, module in pidnet_s.named_modules():\n    if name == target_layer1:\n        module.register_forward_hook(hook_pidnet_s)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mean_standard = np.array([0.485, 0.456, 0.406],dtype = np.float32)\nstd_standard = np.array([0.229, 0.224, 0.225],dtype = np.float32)\nx = (2048 - 200)//2\ny = (1024 - 200)//2\nx_end = x + 200\ny_end = y + 200\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nfor pat in ['pidnet_s']:\n  patch = patches[pat]\n  print(f'Computing for: {pat}')\n  for iter,batches in tqdm(enumerate(val_dataloader)):\n    image_standard,label,_,_,_,idx = batches\n    label_patched = deepcopy(label)\n\n    ## adding patch\n    image_standard[:,:, y:y_end, x:x_end] = patch\n    label_patched[:, y:y_end, x:x_end] = config.train.ignore_label\n    ## On device\n    image_standard = image_standard.to(device)\n    label_patched = label_patched.to(device)\n\n    ##PIDNet-s\n    outputs = pidnet_s(image_standard)\n    size = label.shape\n    output = F.interpolate(\n                  outputs[config.test.output_index_pidnet], size[-2:],\n                  mode='bilinear', align_corners=True\n                          )\n    break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mean_standard = np.array([0.485, 0.456, 0.406],dtype = np.float32)\nstd_standard = np.array([0.229, 0.224, 0.225],dtype = np.float32)\nx = (2048 - 200)//2\ny = (1024 - 200)//2\nx_end = x + 200\ny_end = y + 200\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nfor pat in ['pidnet_s']:\n  patch = patches[pat]\n  print(f'Computing for: {pat}')\n  for iter,batches in tqdm(enumerate(val_dataloader)):\n    image_standard,label,_,_,_,idx = batches\n    label_patched = deepcopy(label)\n\n    ## adding patch\n    patch = torch.rand(3,200,200)\n    image_standard[:,:, y:y_end, x:x_end] = patch\n    label_patched[:, y:y_end, x:x_end] = config.train.ignore_label\n    ## On device\n    image_standard = image_standard.to(device)\n    label_patched = label_patched.to(device)\n\n    ##PIDNet-s\n    outputs = pidnet_s(image_standard)\n    size = label.shape\n    output = F.interpolate(\n                  outputs[config.test.output_index_pidnet], size[-2:],\n                  mode='bilinear', align_corners=True\n                          )\n    break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(feature_maps_pidnet_s_attack))\nprint(len(feature_maps_pidnet_s_attack))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef flatten_and_normalize(fm):\n    # shape: [C, H, W] → [C, H*W]\n    fm_flat = fm.view(fm.shape[0], -1)\n    return F.normalize(fm_flat, dim=1)\n\n# Get feature map of the first image in batch\nfm1 = flatten_and_normalize(feature_maps_pidnet_s[0][0])\nfm2 = flatten_and_normalize(feature_maps_pidnet_s_attack[0][0])\n\n# Compute cosine similarity between corresponding channels\ndef mean_cosine_similarity(fmA, fmB):\n    sim = F.cosine_similarity(fmA, fmB, dim=1)  # channel-wise\n    return sim.mean().item()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Adversarial vs Random:\", mean_cosine_similarity(fm1, fm2))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(fm1-fm2).norm()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fm1.sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training for random location","metadata":{}},{"cell_type":"code","source":"!cp -r /kaggle/input/original-repo/adversarial-patch-transferability /kaggle/working/\n!cp /kaggle/input/pidnet-s-weights/PIDNet_S_Cityscapes_test.pt /kaggle/working/adversarial-patch-transferability/pretrained_models/PIDNet\n!cp /kaggle/input/pidnet-m-weights/PIDNet_M_Cityscapes_test.pt /kaggle/working/adversarial-patch-transferability/pretrained_models/PIDNet\n!cp /kaggle/input/bisenetv2-weights/bisenetv2.pth /kaggle/working/adversarial-patch-transferability/pretrained_models/BisNetV2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working/adversarial-patch-transferability","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import yaml\n\nyaml_path = \"/kaggle/working/adversarial-patch-transferability/configs/config.yaml\"\n\n# Read\nwith open(yaml_path, 'r') as file:\n    conf = yaml.safe_load(file)\n\nprint(conf)  # View content\nconf[\"patch\"][\"loc\"]=\"random\"\nwith open(yaml_path, 'w') as file:\n    yaml.safe_dump(conf, file)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(conf)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport sys\nsys.path.append('/kaggle/working/adversarial-patch-transferability')\nfrom utils.utils import setup_logging, get_config_from_yaml, process_config, print_config\nfrom trainer.trainer import PatchTrainer\nimport torch\nimport pickle","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_names = ['pidnet_s']\n#model_names = ['icnet','bisenet_v1','bisenet_v2','segformer']\n#model_names = ['segformer']\nfor model in model_names:\n  ## getting the config file\n  config = get_config_from_yaml('configs/config.yaml',model = model)\n  #print_config(config)\n\n  ## processing config to initialize directories for logs etc\n  main_logger = process_config(config)\n\n  ## training\n  train_obj = PatchTrainer(config,main_logger)\n  save = train_obj.train()\n  pickle.dump( save, open(config.experiment.log_patch_address+config.model.name+\"_randomloc\"+\".p\", \"wb\" ) )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## BBFA experiment with modified loss on same layers","metadata":{}},{"cell_type":"code","source":"!rm -rf /kaggle/working/adversarial-patch-transferability\n!rm -rf /kaggle/working/state.db\n!rm -rf /kaggle/working/logs\n!rm -rf /kaggle/working/patch\n%cd /kaggle/working","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/SargamG/adversarial-patch-transferability","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cp /kaggle/input/pidnet-s-weights/PIDNet_S_Cityscapes_test.pt /kaggle/working/adversarial-patch-transferability/pretrained_models/PIDNet\n!cp /kaggle/input/pidnet-m-weights/PIDNet_M_Cityscapes_test.pt /kaggle/working/adversarial-patch-transferability/pretrained_models/PIDNet\n!cp /kaggle/input/bisenetv2-weights/bisenetv2.pth /kaggle/working/adversarial-patch-transferability/pretrained_models/BisNetV2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working/adversarial-patch-transferability","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport sys\nsys.path.append('/kaggle/working/adversarial-patch-transferability')\nfrom utils.utils import setup_logging, get_config_from_yaml, process_config, print_config\nfrom trainer.trainer import PatchTrainer\nimport torch\nimport pickle","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = 'pidnet_s'\n## getting the config file\nconfig = get_config_from_yaml('configs/config.yaml',model = model)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n## processing config to initialize directories for logs etc\nmain_logger = process_config(config)\n\n## training\ntrain_obj = PatchTrainer(config,main_logger,model)\ntrain_obj.register_forward_hook1()\ntrain_obj.register_forward_hook2()\nH = torch.load('/kaggle/input/agg-gradient-for-pidnet-s-layer-3/H.pt', map_location = device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"H","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save = train_obj.train(H)\npickle.dump( save, open(config.experiment.log_patch_address+config.model.name+\"_bbfa_modifiedloss\"+\".p\", \"wb\" ) )","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}